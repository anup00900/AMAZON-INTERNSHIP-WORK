{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Quick intro: FLAN-T5, just a better T5\n",
    "\n",
    "FLAN-T5 released with the [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf) paper is an enhanced version of T5 that has been finetuned in a mixture of tasks. The paper explores instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. The paper discovers that overall instruction finetuning is a general method for improving the performance and usability of pretrained language models. \n",
    "\n",
    "![flan-t5](../assets/flan-t5.png)\n",
    "\n",
    "* Paper: https://arxiv.org/abs/2210.11416\n",
    "* Official repo: https://github.com/google-research/t5x\n",
    "\n",
    "--- \n",
    "\n",
    "Now we know what FLAN-T5 is, let's get started. 🚀\n",
    "\n",
    "_Note: This tutorial was created and run on a g4dn.xlarge AWS EC2 Instance including a NVIDIA T4._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_mcqn = pd.read_csv(\"mcq_neighbours.csv\",on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mcqs = pd.read_csv(\"mcq_street.csv\",on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parse = pd.read_csv(\"parse_context_final.csv\",on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_street = pd.read_csv(\"street_normal.csv\",on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scramble_final = pd.read_csv(\"Scramble_FINAL.csv\",on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scramble_word_final = pd.read_csv(\"Shuffling_wordings_FINAL.csv\",on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((82614, 3), (33858, 3), (887195, 3), (4542636, 2), (21954, 2), (21954, 2))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mcqn.shape,df_mcqs.shape,df_parse.shape,df_street.shape,df_scramble_final.shape,df_scramble_word_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O'Brien & Company , 811 First Ave Suite 380 ,  , US , WA , Seattle , 98104  ******  9830 overlook Dr  ,  ,  , US , Wa , Olympia , 98502  |  O'BRIEN & COMPANY , 811 1ST AVE STE 380 ,  , US , WA , SEATTLE , 98104-1434   |   O'Brien & Company , 811 First Avenue  Suite 380 ,  , US , WA , SEATTLE , 98104 ******  O'BRIEN & COMPANY , 811 1ST AVE STE 380 ,  , US , WA , SEATTLE , 98104-1434  |  O'Brien & Company , 811 First Avenue  Suite 380 ,  , US , WA , SEATTLE , 98104\n"
     ]
    }
   ],
   "source": [
    "print(df_mcqn['input'][0],\"******\",df_mcqn['context'][0],\"******\",df_mcqn['target'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 100TH AVE SE ,  ,  , US , WA , OLYMPIA , 98501-9710  ******  608 W EMERSON ST APT 321 ,  ,  , US , WA , SEATTLE , 98119-1544   |   805 W EMERSON ST ,  ,  , US , WA , SEATTLE , 98119-1457  |  434 100TH AVE SE ,  ,  , US , WA , OLYMPIA , 98501  ******  434 100TH AVE SE ,  ,  , US , WA , OLYMPIA , 98501 \n"
     ]
    }
   ],
   "source": [
    "print(df_mcqs['input'][0],\"******\",df_mcqs['context'][0],\"******\",df_mcqs['target'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133 18TH AVE APT 4|||WA|SEATTLE|98122 ****** STREET:18TH AVE;BUILDING:1133;UNIT:APT|4;CITY:SEATTLE;STATE:WA;POSTALCODE:98122 4701 ****** 1133 18TH AVE APT 4 ,  ,  , US , WA , SEATTLE , 98122  |  1133 18TH AVE APT 4 ,  ,  , US , WA , SEATTLE , 98122-4701 \n"
     ]
    }
   ],
   "source": [
    "print(df_parse['prompt'][0],\"******\",df_parse['response'][0],\"******\",df_parse['context'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_text</th>\n",
       "      <th>input_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100TH_AVE_SE</td>\n",
       "      <td>612 100th Ave se , Olympia ,  , US , WA , OLYM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100TH_AVE_SE</td>\n",
       "      <td>434 100th Avenue SE ,  ,  , US , WA , Olympia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100TH_AVE_SE</td>\n",
       "      <td>434 100th Ave. S.E. ,  ,  , US , Washington , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100TH_AVE_SE</td>\n",
       "      <td>424 100th ave se ,  ,  , US , washington , tum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100TH_AVE_SE</td>\n",
       "      <td>424 100th Ave. SE ,  ,  , US , Washington , Ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542631</th>\n",
       "      <td>Z_ST_SE</td>\n",
       "      <td>242 Z ST SE ,  ,  , US , WA , TUMWATER , 98501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542632</th>\n",
       "      <td>Z_ST_SE</td>\n",
       "      <td>201 Z ST SE ,  ,  , US , WA , Tumwater , 98501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542633</th>\n",
       "      <td>Z_ST_SE</td>\n",
       "      <td>230 Z ST SE ,  ,  , US , WA , TUMWATER , 98501...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542634</th>\n",
       "      <td>Z_ST_SE</td>\n",
       "      <td>709 Z ST SE ,  ,  , US , WA , OLYMPIA , 98501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542635</th>\n",
       "      <td>Z_ST_SE</td>\n",
       "      <td>207 Z ST SE ,  ,  , US , WA , TUMWATER , 98501...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4542636 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          target_text                                         input_text\n",
       "0        100TH_AVE_SE  612 100th Ave se , Olympia ,  , US , WA , OLYM...\n",
       "1        100TH_AVE_SE  434 100th Avenue SE ,  ,  , US , WA , Olympia ...\n",
       "2        100TH_AVE_SE  434 100th Ave. S.E. ,  ,  , US , Washington , ...\n",
       "3        100TH_AVE_SE  424 100th ave se ,  ,  , US , washington , tum...\n",
       "4        100TH_AVE_SE  424 100th Ave. SE ,  ,  , US , Washington , Ol...\n",
       "...               ...                                                ...\n",
       "4542631       Z_ST_SE     242 Z ST SE ,  ,  , US , WA , TUMWATER , 98501\n",
       "4542632       Z_ST_SE     201 Z ST SE ,  ,  , US , WA , Tumwater , 98501\n",
       "4542633       Z_ST_SE  230 Z ST SE ,  ,  , US , WA , TUMWATER , 98501...\n",
       "4542634       Z_ST_SE      709 Z ST SE ,  ,  , US , WA , OLYMPIA , 98501\n",
       "4542635       Z_ST_SE  207 Z ST SE ,  ,  , US , WA , TUMWATER , 98501...\n",
       "\n",
       "[4542636 rows x 2 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_street.rename(columns = {'input':'input_text'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scramble_final.rename(columns = {'scramble_input':'input_text'}, inplace = True)\n",
    "df_scramble_final.rename(columns = {'target':'target_text'}, inplace = True)\n",
    "df_scramble_word_final.rename(columns = {'shuffle_input':'input_text'}, inplace = True)\n",
    "df_scramble_word_final.rename(columns = {'target':'target_text'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_text</th>\n",
       "      <th>input_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111 YALE AVE N , APT212 , SEATTLE , WA , US</td>\n",
       "      <td>111 EYAL EVA N , PTA212 , LEETAST , AW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1121 Harrison Avenue  , #115 , Centralia  , WA...</td>\n",
       "      <td>1121 OARSRHIN AVE # 115 ,  , RALNIACET , AW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5000 25th Ave NE , Apt 2207-C , Seattle , WA , US</td>\n",
       "      <td>5000 25th Aev NE , Tpa 2207-C , Lttseea , AW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11625 center road , unit d , EVERETT , WA , US</td>\n",
       "      <td>11625 NCRETE RD NITU D ,  , VEETRET , WA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1020 E Jefferson St , 325 , SEATTLE , WA , US</td>\n",
       "      <td>1020 E FJEESRNOF TS 325 ,  , SETAETL , WA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21949</th>\n",
       "      <td>3025 Limited Ln NW , Ste 100 , Olympia , WA , US</td>\n",
       "      <td>3025 ILTMEDI LN WN TES 100 ,  , OPAMIYL , WA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21950</th>\n",
       "      <td>17424 122nd AVE E , Apt. B105 , Puyallup , WA ...</td>\n",
       "      <td>17424 122dn EVA E , Tap. B105 , Laupyplu , AW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21951</th>\n",
       "      <td>3013 99th Ave NE , Unit A , Lake Stevens , WA ...</td>\n",
       "      <td>3013 99TH AVE EN TNUI A ,  , LEAK NVSTEES , WA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21952</th>\n",
       "      <td>5240 UNIVERSITY WAY NE , APT607 , SEATTLE , WA...</td>\n",
       "      <td>5240 RINSIYETUV YWA EN , PTA607 , EASTTLE , AW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21953</th>\n",
       "      <td>4721 38TH AVE SW , B205 , SEATTLE , WA , US</td>\n",
       "      <td>4721 38TH VAE SW , B205 , TEEALTS , AW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21954 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             target_text  \\\n",
       "0            111 YALE AVE N , APT212 , SEATTLE , WA , US   \n",
       "1      1121 Harrison Avenue  , #115 , Centralia  , WA...   \n",
       "2      5000 25th Ave NE , Apt 2207-C , Seattle , WA , US   \n",
       "3         11625 center road , unit d , EVERETT , WA , US   \n",
       "4          1020 E Jefferson St , 325 , SEATTLE , WA , US   \n",
       "...                                                  ...   \n",
       "21949   3025 Limited Ln NW , Ste 100 , Olympia , WA , US   \n",
       "21950  17424 122nd AVE E , Apt. B105 , Puyallup , WA ...   \n",
       "21951  3013 99th Ave NE , Unit A , Lake Stevens , WA ...   \n",
       "21952  5240 UNIVERSITY WAY NE , APT607 , SEATTLE , WA...   \n",
       "21953        4721 38TH AVE SW , B205 , SEATTLE , WA , US   \n",
       "\n",
       "                                           input_text  \n",
       "0              111 EYAL EVA N , PTA212 , LEETAST , AW  \n",
       "1         1121 OARSRHIN AVE # 115 ,  , RALNIACET , AW  \n",
       "2        5000 25th Aev NE , Tpa 2207-C , Lttseea , AW  \n",
       "3            11625 NCRETE RD NITU D ,  , VEETRET , WA  \n",
       "4           1020 E FJEESRNOF TS 325 ,  , SETAETL , WA  \n",
       "...                                               ...  \n",
       "21949    3025 ILTMEDI LN WN TES 100 ,  , OPAMIYL , WA  \n",
       "21950   17424 122dn EVA E , Tap. B105 , Laupyplu , AW  \n",
       "21951  3013 99TH AVE EN TNUI A ,  , LEAK NVSTEES , WA  \n",
       "21952  5240 RINSIYETUV YWA EN , PTA607 , EASTTLE , AW  \n",
       "21953          4721 38TH VAE SW , B205 , TEEALTS , AW  \n",
       "\n",
       "[21954 rows x 2 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scramble_word_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb = pd.concat([df_scramble_word_final,df_scramble_final],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_text</th>\n",
       "      <th>input_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111 YALE AVE N , APT212 , SEATTLE , WA , US</td>\n",
       "      <td>111 EYAL EVA N , PTA212 , LEETAST , AW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1121 Harrison Avenue  , #115 , Centralia  , WA...</td>\n",
       "      <td>1121 OARSRHIN AVE # 115 ,  , RALNIACET , AW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5000 25th Ave NE , Apt 2207-C , Seattle , WA , US</td>\n",
       "      <td>5000 25th Aev NE , Tpa 2207-C , Lttseea , AW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11625 center road , unit d , EVERETT , WA , US</td>\n",
       "      <td>11625 NCRETE RD NITU D ,  , VEETRET , WA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1020 E Jefferson St , 325 , SEATTLE , WA , US</td>\n",
       "      <td>1020 E FJEESRNOF TS 325 ,  , SETAETL , WA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21949</th>\n",
       "      <td>3025 Limited Ln NW , Ste 100 , Olympia , WA , US</td>\n",
       "      <td>OLYMPIA WA 100 , NW , LN STE 3025 , LIMITED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21950</th>\n",
       "      <td>17424 122nd AVE E , Apt. B105 , Puyallup , WA ...</td>\n",
       "      <td>E 17424 , , Puyallup Apt. , AVE WA 122nd B105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21951</th>\n",
       "      <td>3013 99th Ave NE , Unit A , Lake Stevens , WA ...</td>\n",
       "      <td>, STEVENS AVE LAKE UNIT 99TH NE , A , WA 3013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21952</th>\n",
       "      <td>5240 UNIVERSITY WAY NE , APT607 , SEATTLE , WA...</td>\n",
       "      <td>NE APT607 UNIVERSITY , SEATTLE 5240 , WA , WAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21953</th>\n",
       "      <td>4721 38TH AVE SW , B205 , SEATTLE , WA , US</td>\n",
       "      <td>AVE , SW WA B205 , 4721 38TH SEATTLE ,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43908 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             target_text  \\\n",
       "0            111 YALE AVE N , APT212 , SEATTLE , WA , US   \n",
       "1      1121 Harrison Avenue  , #115 , Centralia  , WA...   \n",
       "2      5000 25th Ave NE , Apt 2207-C , Seattle , WA , US   \n",
       "3         11625 center road , unit d , EVERETT , WA , US   \n",
       "4          1020 E Jefferson St , 325 , SEATTLE , WA , US   \n",
       "...                                                  ...   \n",
       "21949   3025 Limited Ln NW , Ste 100 , Olympia , WA , US   \n",
       "21950  17424 122nd AVE E , Apt. B105 , Puyallup , WA ...   \n",
       "21951  3013 99th Ave NE , Unit A , Lake Stevens , WA ...   \n",
       "21952  5240 UNIVERSITY WAY NE , APT607 , SEATTLE , WA...   \n",
       "21953        4721 38TH AVE SW , B205 , SEATTLE , WA , US   \n",
       "\n",
       "                                           input_text  \n",
       "0              111 EYAL EVA N , PTA212 , LEETAST , AW  \n",
       "1         1121 OARSRHIN AVE # 115 ,  , RALNIACET , AW  \n",
       "2        5000 25th Aev NE , Tpa 2207-C , Lttseea , AW  \n",
       "3            11625 NCRETE RD NITU D ,  , VEETRET , WA  \n",
       "4           1020 E FJEESRNOF TS 325 ,  , SETAETL , WA  \n",
       "...                                               ...  \n",
       "21949     OLYMPIA WA 100 , NW , LN STE 3025 , LIMITED  \n",
       "21950   E 17424 , , Puyallup Apt. , AVE WA 122nd B105  \n",
       "21951   , STEVENS AVE LAKE UNIT 99TH NE , A , WA 3013  \n",
       "21952  NE APT607 UNIVERSITY , SEATTLE 5240 , WA , WAY  \n",
       "21953          AVE , SW WA B205 , 4721 38TH SEATTLE ,  \n",
       "\n",
       "[43908 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comb #scrable word and sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((82614, 3), (33858, 3), (887195, 3))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mcqn.shape,df_mcqs.shape,df_parse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O'Brien &amp; Company , 811 First Ave Suite 380 , ...</td>\n",
       "      <td>9830 overlook Dr  ,  ,  , US , Wa , Olympia ,...</td>\n",
       "      <td>O'BRIEN &amp; COMPANY , 811 1ST AVE STE 380 ,  , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O'BRIEN &amp; COMPANY , 811 1ST AVE STE 380 ,  , ...</td>\n",
       "      <td>2502 ISLAND DR NW ,  ,  , US , WA , OLYMPIA ...</td>\n",
       "      <td>O'Brien &amp; Company , 811 First Ave Suite 380 , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O'Brien &amp; Company , 811 First Avenue  Suite 3...</td>\n",
       "      <td>3614 12TH AVE W ,  ,  , US , WA , SEATTLE , 98...</td>\n",
       "      <td>O'Brien &amp; Company , 811 First Ave Suite 380 , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>711 78th Ave SW ,  ,  , US , WA , Tumwater , 9...</td>\n",
       "      <td>711 78th Ave SW ,  ,  , US , WA , Tumwater ,...</td>\n",
       "      <td>c/o  Mud Bay , 711 78th Ave SW ,  , US , WA ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c/o  Mud Bay , 711 78th Ave SW ,  , US , WA ,...</td>\n",
       "      <td>5506 CAMELOT DR SW ,  ,  , US , WA , OLYMPIA ,...</td>\n",
       "      <td>711 78th Ave SW ,  ,  , US , WA , Tumwater , 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82609</th>\n",
       "      <td>101 6TH AVE S UNIT 313 ,  ,  , US , WA , SEAT...</td>\n",
       "      <td>2428 NW MARKET ST APT 662 ,  ,  , US , WA , S...</td>\n",
       "      <td>101 6TH AVE S UNIT 313 ,  ,  , US , WA , SEATT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82610</th>\n",
       "      <td>101 6TH AVE S STREET UNIT 313 ,  ,  , US , WA...</td>\n",
       "      <td>101 6TH AVE S UNIT 313 ,  ,  , US , WA , SEATT...</td>\n",
       "      <td>101 6TH AVE S UNIT 313 ,  ,  , US , WA , SEATT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82611</th>\n",
       "      <td>101 6th Ave S # 319 ,  ,  , US , WA , Seattle ...</td>\n",
       "      <td>6943 BIRDSEYE AVE NE UNIT 201 ,  ,  , US , WA ...</td>\n",
       "      <td>Crowd Cow , 101 6th Ave S Apt 319 ,  , US , W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82612</th>\n",
       "      <td>Crowd Cow , 101 6th Ave S Apt 319 ,  , US , W...</td>\n",
       "      <td>101 6th Ave S # 319 ,  ,  , US , WA , Seattle ...</td>\n",
       "      <td>101 6th Ave S # 319 ,  ,  , US , WA , Seattle ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82613</th>\n",
       "      <td>101 6TH AVE S UNIT 319 ,  ,  , US , WA , SEAT...</td>\n",
       "      <td>101 6th Ave S # 319 ,  ,  , US , WA , Seattle ...</td>\n",
       "      <td>101 6th Ave S # 319 ,  ,  , US , WA , Seattle ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82614 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input  \\\n",
       "0      O'Brien & Company , 811 First Ave Suite 380 , ...   \n",
       "1       O'BRIEN & COMPANY , 811 1ST AVE STE 380 ,  , ...   \n",
       "2       O'Brien & Company , 811 First Avenue  Suite 3...   \n",
       "3      711 78th Ave SW ,  ,  , US , WA , Tumwater , 9...   \n",
       "4       c/o  Mud Bay , 711 78th Ave SW ,  , US , WA ,...   \n",
       "...                                                  ...   \n",
       "82609   101 6TH AVE S UNIT 313 ,  ,  , US , WA , SEAT...   \n",
       "82610   101 6TH AVE S STREET UNIT 313 ,  ,  , US , WA...   \n",
       "82611  101 6th Ave S # 319 ,  ,  , US , WA , Seattle ...   \n",
       "82612   Crowd Cow , 101 6th Ave S Apt 319 ,  , US , W...   \n",
       "82613   101 6TH AVE S UNIT 319 ,  ,  , US , WA , SEAT...   \n",
       "\n",
       "                                                 context  \\\n",
       "0       9830 overlook Dr  ,  ,  , US , Wa , Olympia ,...   \n",
       "1        2502 ISLAND DR NW ,  ,  , US , WA , OLYMPIA ...   \n",
       "2      3614 12TH AVE W ,  ,  , US , WA , SEATTLE , 98...   \n",
       "3        711 78th Ave SW ,  ,  , US , WA , Tumwater ,...   \n",
       "4      5506 CAMELOT DR SW ,  ,  , US , WA , OLYMPIA ,...   \n",
       "...                                                  ...   \n",
       "82609   2428 NW MARKET ST APT 662 ,  ,  , US , WA , S...   \n",
       "82610  101 6TH AVE S UNIT 313 ,  ,  , US , WA , SEATT...   \n",
       "82611  6943 BIRDSEYE AVE NE UNIT 201 ,  ,  , US , WA ...   \n",
       "82612  101 6th Ave S # 319 ,  ,  , US , WA , Seattle ...   \n",
       "82613  101 6th Ave S # 319 ,  ,  , US , WA , Seattle ...   \n",
       "\n",
       "                                                  target  \n",
       "0       O'BRIEN & COMPANY , 811 1ST AVE STE 380 ,  , ...  \n",
       "1      O'Brien & Company , 811 First Ave Suite 380 , ...  \n",
       "2      O'Brien & Company , 811 First Ave Suite 380 , ...  \n",
       "3       c/o  Mud Bay , 711 78th Ave SW ,  , US , WA ,...  \n",
       "4      711 78th Ave SW ,  ,  , US , WA , Tumwater , 9...  \n",
       "...                                                  ...  \n",
       "82609  101 6TH AVE S UNIT 313 ,  ,  , US , WA , SEATT...  \n",
       "82610  101 6TH AVE S UNIT 313 ,  ,  , US , WA , SEATT...  \n",
       "82611   Crowd Cow , 101 6th Ave S Apt 319 ,  , US , W...  \n",
       "82612  101 6th Ave S # 319 ,  ,  , US , WA , Seattle ...  \n",
       "82613  101 6th Ave S # 319 ,  ,  , US , WA , Seattle ...  \n",
       "\n",
       "[82614 rows x 3 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mcqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Input to Capital letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mcqs = df_mcqs.apply(lambda x: x.astype(str).str.upper())\n",
    "df_parse = df_parse.apply(lambda x: x.astype(str).str.upper())\n",
    "df_street = df_street.apply(lambda x: x.astype(str).str.upper())\n",
    "df_mcqn = df_mcqn.apply(lambda x: x.astype(str).str.upper())\n",
    "df_comb = df_comb.apply(lambda x: x.astype(str).str.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_mcqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mcqs['input'] = 'Match the given address in input text to addresses in CONTEXT based on street: ' + df_mcqs['input'].astype(str)\n",
    "df_mcqs['context'] = 'CONTEXT - ' + df_mcqs['context'].astype(str)\n",
    "df_mcqs[\"input_text\"] = df_mcqs[\"input\"].astype(str) +\"  ,  \"+ df_mcqs[\"context\"].astype(str)\n",
    "df_mcqs['target_text'] = 'OUTPUT : ' + df_mcqs['target'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mcqs= df_mcqs.drop(['input','context','target'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Match the given address in input text to addresses in CONTEXT based on street: 424 100TH AVE SE ,  ,  , US , WA , OLYMPIA , 98501-9710   ,  CONTEXT -  608 W EMERSON ST APT 321 ,  ,  , US , WA , SEATTLE , 98119-1544   |   805 W EMERSON ST ,  ,  , US , WA , SEATTLE , 98119-1457  |  434 100TH AVE SE ,  ,  , US , WA , OLYMPIA , 98501 '"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mcqs['input_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mcqn['input'] = 'Match the given address in input text to addresses in CONTEXT based on neighbour:' + df_mcqn['input'].astype(str)\n",
    "df_mcqn['context'] = 'CONTEXT - ' + df_mcqn['context'].astype(str)\n",
    "df_mcqn[\"input_text\"] = df_mcqn[\"input\"].astype(str) +\"  ,  \"+ df_mcqn[\"context\"].astype(str)\n",
    "df_mcqn['target_text'] = 'OUTPUT : ' + df_mcqn['target'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mcqn= df_mcqn.drop(['input','context','target'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Match the given address in input text to addre...</td>\n",
       "      <td>OUTPUT :  O'BRIEN &amp; COMPANY , 811 1ST AVE STE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Match the given address in input text to addre...</td>\n",
       "      <td>OUTPUT : O'BRIEN &amp; COMPANY , 811 FIRST AVE SUI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Match the given address in input text to addre...</td>\n",
       "      <td>OUTPUT : O'BRIEN &amp; COMPANY , 811 FIRST AVE SUI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Match the given address in input text to addre...</td>\n",
       "      <td>OUTPUT :  C/O  MUD BAY , 711 78TH AVE SW ,  , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Match the given address in input text to addre...</td>\n",
       "      <td>OUTPUT : 711 78TH AVE SW ,  ,  , US , WA , TUM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82609</th>\n",
       "      <td>Match the given address in input text to addre...</td>\n",
       "      <td>OUTPUT : 101 6TH AVE S UNIT 313 ,  ,  , US , W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82610</th>\n",
       "      <td>Match the given address in input text to addre...</td>\n",
       "      <td>OUTPUT : 101 6TH AVE S UNIT 313 ,  ,  , US , W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82611</th>\n",
       "      <td>Match the given address in input text to addre...</td>\n",
       "      <td>OUTPUT :  CROWD COW , 101 6TH AVE S APT 319 , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82612</th>\n",
       "      <td>Match the given address in input text to addre...</td>\n",
       "      <td>OUTPUT : 101 6TH AVE S # 319 ,  ,  , US , WA ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82613</th>\n",
       "      <td>Match the given address in input text to addre...</td>\n",
       "      <td>OUTPUT : 101 6TH AVE S # 319 ,  ,  , US , WA ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82614 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              input_text  \\\n",
       "0      Match the given address in input text to addre...   \n",
       "1      Match the given address in input text to addre...   \n",
       "2      Match the given address in input text to addre...   \n",
       "3      Match the given address in input text to addre...   \n",
       "4      Match the given address in input text to addre...   \n",
       "...                                                  ...   \n",
       "82609  Match the given address in input text to addre...   \n",
       "82610  Match the given address in input text to addre...   \n",
       "82611  Match the given address in input text to addre...   \n",
       "82612  Match the given address in input text to addre...   \n",
       "82613  Match the given address in input text to addre...   \n",
       "\n",
       "                                             target_text  \n",
       "0      OUTPUT :  O'BRIEN & COMPANY , 811 1ST AVE STE ...  \n",
       "1      OUTPUT : O'BRIEN & COMPANY , 811 FIRST AVE SUI...  \n",
       "2      OUTPUT : O'BRIEN & COMPANY , 811 FIRST AVE SUI...  \n",
       "3      OUTPUT :  C/O  MUD BAY , 711 78TH AVE SW ,  , ...  \n",
       "4      OUTPUT : 711 78TH AVE SW ,  ,  , US , WA , TUM...  \n",
       "...                                                  ...  \n",
       "82609  OUTPUT : 101 6TH AVE S UNIT 313 ,  ,  , US , W...  \n",
       "82610  OUTPUT : 101 6TH AVE S UNIT 313 ,  ,  , US , W...  \n",
       "82611  OUTPUT :  CROWD COW , 101 6TH AVE S APT 319 , ...  \n",
       "82612  OUTPUT : 101 6TH AVE S # 319 ,  ,  , US , WA ,...  \n",
       "82613  OUTPUT : 101 6TH AVE S # 319 ,  ,  , US , WA ,...  \n",
       "\n",
       "[82614 rows x 2 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mcqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parse['prompt'] = 'Parse the given address based on CONTEXT: ' + df_parse['prompt'].astype(str)\n",
    "df_parse['context'] = 'CONTEXT - ' + df_parse['context'].astype(str)\n",
    "df_parse[\"input_text\"] = df_parse[\"prompt\"].astype(str) +\"  ,  \" + df_parse[\"context\"].astype(str)\n",
    "df_parse['target_text'] = 'OUTPUT : ' + df_parse['response'].astype(str)\n",
    "df_parse = df_parse.drop(['prompt','response','context'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Parse the given address based on CONTEXT: 1133...</td>\n",
       "      <td>OUTPUT : STREET:18TH AVE;BUILDING:1133;UNIT:AP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Parse the given address based on CONTEXT: 1133...</td>\n",
       "      <td>OUTPUT : STREET:18TH AVE;BUILDING:1133;UNIT:AP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Parse the given address based on CONTEXT: 1133...</td>\n",
       "      <td>OUTPUT : STREET:18TH AVE;BUILDING:1133;UNIT:AP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parse the given address based on CONTEXT: 1133...</td>\n",
       "      <td>OUTPUT : STREET:18TH AVE;BUILDING:1133;UNIT:AP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Parse the given address based on CONTEXT: !113...</td>\n",
       "      <td>OUTPUT : STREET:18TH AVE;BUILDING:1133;UNIT:AP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887190</th>\n",
       "      <td>Parse the given address based on CONTEXT: 1201...</td>\n",
       "      <td>OUTPUT : STREET:NULL;BUILDING:NULL;UNIT:NULL;C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887191</th>\n",
       "      <td>Parse the given address based on CONTEXT: 4636...</td>\n",
       "      <td>OUTPUT : STREET:NULL;BUILDING:NULL;UNIT:NULL;C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887192</th>\n",
       "      <td>Parse the given address based on CONTEXT: E 90...</td>\n",
       "      <td>OUTPUT : STREET:NULL;BUILDING:NULL;UNIT:NULL;C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887193</th>\n",
       "      <td>Parse the given address based on CONTEXT: |355...</td>\n",
       "      <td>OUTPUT : STREET:NULL;BUILDING:NULL;UNIT:NULL;C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887194</th>\n",
       "      <td>Parse the given address based on CONTEXT:   51...</td>\n",
       "      <td>OUTPUT : STREET:NULL;BUILDING:NULL;UNIT:NULL;C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>887195 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input_text  \\\n",
       "0       Parse the given address based on CONTEXT: 1133...   \n",
       "1       Parse the given address based on CONTEXT: 1133...   \n",
       "2       Parse the given address based on CONTEXT: 1133...   \n",
       "3       Parse the given address based on CONTEXT: 1133...   \n",
       "4       Parse the given address based on CONTEXT: !113...   \n",
       "...                                                   ...   \n",
       "887190  Parse the given address based on CONTEXT: 1201...   \n",
       "887191  Parse the given address based on CONTEXT: 4636...   \n",
       "887192  Parse the given address based on CONTEXT: E 90...   \n",
       "887193  Parse the given address based on CONTEXT: |355...   \n",
       "887194  Parse the given address based on CONTEXT:   51...   \n",
       "\n",
       "                                              target_text  \n",
       "0       OUTPUT : STREET:18TH AVE;BUILDING:1133;UNIT:AP...  \n",
       "1       OUTPUT : STREET:18TH AVE;BUILDING:1133;UNIT:AP...  \n",
       "2       OUTPUT : STREET:18TH AVE;BUILDING:1133;UNIT:AP...  \n",
       "3       OUTPUT : STREET:18TH AVE;BUILDING:1133;UNIT:AP...  \n",
       "4       OUTPUT : STREET:18TH AVE;BUILDING:1133;UNIT:AP...  \n",
       "...                                                   ...  \n",
       "887190  OUTPUT : STREET:NULL;BUILDING:NULL;UNIT:NULL;C...  \n",
       "887191  OUTPUT : STREET:NULL;BUILDING:NULL;UNIT:NULL;C...  \n",
       "887192  OUTPUT : STREET:NULL;BUILDING:NULL;UNIT:NULL;C...  \n",
       "887193  OUTPUT : STREET:NULL;BUILDING:NULL;UNIT:NULL;C...  \n",
       "887194  OUTPUT : STREET:NULL;BUILDING:NULL;UNIT:NULL;C...  \n",
       "\n",
       "[887195 rows x 2 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_street['input_text'] = 'Find street in given address: '+df_street['input_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_street['target_text'] = 'OUTPUT : ' + df_street['target_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_text</th>\n",
       "      <th>input_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OUTPUT : 100TH_AVE_SE</td>\n",
       "      <td>Find street in given address: 612 100TH AVE SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OUTPUT : 100TH_AVE_SE</td>\n",
       "      <td>Find street in given address: 434 100TH AVENUE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OUTPUT : 100TH_AVE_SE</td>\n",
       "      <td>Find street in given address: 434 100TH AVE. S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OUTPUT : 100TH_AVE_SE</td>\n",
       "      <td>Find street in given address: 424 100TH AVE SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OUTPUT : 100TH_AVE_SE</td>\n",
       "      <td>Find street in given address: 424 100TH AVE. S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542631</th>\n",
       "      <td>OUTPUT : Z_ST_SE</td>\n",
       "      <td>Find street in given address: 242 Z ST SE ,  ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542632</th>\n",
       "      <td>OUTPUT : Z_ST_SE</td>\n",
       "      <td>Find street in given address: 201 Z ST SE ,  ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542633</th>\n",
       "      <td>OUTPUT : Z_ST_SE</td>\n",
       "      <td>Find street in given address: 230 Z ST SE ,  ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542634</th>\n",
       "      <td>OUTPUT : Z_ST_SE</td>\n",
       "      <td>Find street in given address: 709 Z ST SE ,  ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542635</th>\n",
       "      <td>OUTPUT : Z_ST_SE</td>\n",
       "      <td>Find street in given address: 207 Z ST SE ,  ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4542636 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   target_text  \\\n",
       "0        OUTPUT : 100TH_AVE_SE   \n",
       "1        OUTPUT : 100TH_AVE_SE   \n",
       "2        OUTPUT : 100TH_AVE_SE   \n",
       "3        OUTPUT : 100TH_AVE_SE   \n",
       "4        OUTPUT : 100TH_AVE_SE   \n",
       "...                        ...   \n",
       "4542631       OUTPUT : Z_ST_SE   \n",
       "4542632       OUTPUT : Z_ST_SE   \n",
       "4542633       OUTPUT : Z_ST_SE   \n",
       "4542634       OUTPUT : Z_ST_SE   \n",
       "4542635       OUTPUT : Z_ST_SE   \n",
       "\n",
       "                                                input_text  \n",
       "0        Find street in given address: 612 100TH AVE SE...  \n",
       "1        Find street in given address: 434 100TH AVENUE...  \n",
       "2        Find street in given address: 434 100TH AVE. S...  \n",
       "3        Find street in given address: 424 100TH AVE SE...  \n",
       "4        Find street in given address: 424 100TH AVE. S...  \n",
       "...                                                    ...  \n",
       "4542631  Find street in given address: 242 Z ST SE ,  ,...  \n",
       "4542632  Find street in given address: 201 Z ST SE ,  ,...  \n",
       "4542633  Find street in given address: 230 Z ST SE ,  ,...  \n",
       "4542634  Find street in given address: 709 Z ST SE ,  ,...  \n",
       "4542635  Find street in given address: 207 Z ST SE ,  ,...  \n",
       "\n",
       "[4542636 rows x 2 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb['input_text'] = 'Correct the address: '+df_comb['input_text'].astype(str)\n",
    "df_comb['target_text'] = 'OUTPUT : ' + df_comb['target_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_text</th>\n",
       "      <th>input_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OUTPUT : 111 YALE AVE N , APT212 , SEATTLE , W...</td>\n",
       "      <td>Correct the address: 111 EYAL EVA N , PTA212 ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OUTPUT : 1121 HARRISON AVENUE  , #115 , CENTRA...</td>\n",
       "      <td>Correct the address: 1121 OARSRHIN AVE # 115 ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OUTPUT : 5000 25TH AVE NE , APT 2207-C , SEATT...</td>\n",
       "      <td>Correct the address: 5000 25TH AEV NE , TPA 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OUTPUT : 11625 CENTER ROAD , UNIT D , EVERETT ...</td>\n",
       "      <td>Correct the address: 11625 NCRETE RD NITU D , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OUTPUT : 1020 E JEFFERSON ST , 325 , SEATTLE ,...</td>\n",
       "      <td>Correct the address: 1020 E FJEESRNOF TS 325 ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21949</th>\n",
       "      <td>OUTPUT : 3025 LIMITED LN NW , STE 100 , OLYMPI...</td>\n",
       "      <td>Correct the address: OLYMPIA WA 100 , NW , LN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21950</th>\n",
       "      <td>OUTPUT : 17424 122ND AVE E , APT. B105 , PUYAL...</td>\n",
       "      <td>Correct the address: E 17424 , , PUYALLUP APT....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21951</th>\n",
       "      <td>OUTPUT : 3013 99TH AVE NE , UNIT A , LAKE STEV...</td>\n",
       "      <td>Correct the address: , STEVENS AVE LAKE UNIT 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21952</th>\n",
       "      <td>OUTPUT : 5240 UNIVERSITY WAY NE , APT607 , SEA...</td>\n",
       "      <td>Correct the address: NE APT607 UNIVERSITY , SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21953</th>\n",
       "      <td>OUTPUT : 4721 38TH AVE SW , B205 , SEATTLE , W...</td>\n",
       "      <td>Correct the address: AVE , SW WA B205 , 4721 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43908 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             target_text  \\\n",
       "0      OUTPUT : 111 YALE AVE N , APT212 , SEATTLE , W...   \n",
       "1      OUTPUT : 1121 HARRISON AVENUE  , #115 , CENTRA...   \n",
       "2      OUTPUT : 5000 25TH AVE NE , APT 2207-C , SEATT...   \n",
       "3      OUTPUT : 11625 CENTER ROAD , UNIT D , EVERETT ...   \n",
       "4      OUTPUT : 1020 E JEFFERSON ST , 325 , SEATTLE ,...   \n",
       "...                                                  ...   \n",
       "21949  OUTPUT : 3025 LIMITED LN NW , STE 100 , OLYMPI...   \n",
       "21950  OUTPUT : 17424 122ND AVE E , APT. B105 , PUYAL...   \n",
       "21951  OUTPUT : 3013 99TH AVE NE , UNIT A , LAKE STEV...   \n",
       "21952  OUTPUT : 5240 UNIVERSITY WAY NE , APT607 , SEA...   \n",
       "21953  OUTPUT : 4721 38TH AVE SW , B205 , SEATTLE , W...   \n",
       "\n",
       "                                              input_text  \n",
       "0      Correct the address: 111 EYAL EVA N , PTA212 ,...  \n",
       "1      Correct the address: 1121 OARSRHIN AVE # 115 ,...  \n",
       "2      Correct the address: 5000 25TH AEV NE , TPA 22...  \n",
       "3      Correct the address: 11625 NCRETE RD NITU D , ...  \n",
       "4      Correct the address: 1020 E FJEESRNOF TS 325 ,...  \n",
       "...                                                  ...  \n",
       "21949  Correct the address: OLYMPIA WA 100 , NW , LN ...  \n",
       "21950  Correct the address: E 17424 , , PUYALLUP APT....  \n",
       "21951  Correct the address: , STEVENS AVE LAKE UNIT 9...  \n",
       "21952  Correct the address: NE APT607 UNIVERSITY , SE...  \n",
       "21953  Correct the address: AVE , SW WA B205 , 4721 3...  \n",
       "\n",
       "[43908 rows x 2 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ans = pd.concat([df_mcqn,df_mcqs,df_parse,df_comb,df_street]).astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Match the given address in input text to addre...</td>\n",
       "      <td>OUTPUT :  O'BRIEN &amp; COMPANY , 811 1ST AVE STE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Match the given address in input text to addre...</td>\n",
       "      <td>OUTPUT : O'BRIEN &amp; COMPANY , 811 FIRST AVE SUI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Match the given address in input text to addre...</td>\n",
       "      <td>OUTPUT : O'BRIEN &amp; COMPANY , 811 FIRST AVE SUI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Match the given address in input text to addre...</td>\n",
       "      <td>OUTPUT :  C/O  MUD BAY , 711 78TH AVE SW ,  , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Match the given address in input text to addre...</td>\n",
       "      <td>OUTPUT : 711 78TH AVE SW ,  ,  , US , WA , TUM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542631</th>\n",
       "      <td>Find street in given address: 242 Z ST SE ,  ,...</td>\n",
       "      <td>OUTPUT : Z_ST_SE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542632</th>\n",
       "      <td>Find street in given address: 201 Z ST SE ,  ,...</td>\n",
       "      <td>OUTPUT : Z_ST_SE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542633</th>\n",
       "      <td>Find street in given address: 230 Z ST SE ,  ,...</td>\n",
       "      <td>OUTPUT : Z_ST_SE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542634</th>\n",
       "      <td>Find street in given address: 709 Z ST SE ,  ,...</td>\n",
       "      <td>OUTPUT : Z_ST_SE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542635</th>\n",
       "      <td>Find street in given address: 207 Z ST SE ,  ,...</td>\n",
       "      <td>OUTPUT : Z_ST_SE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5590211 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input_text  \\\n",
       "0        Match the given address in input text to addre...   \n",
       "1        Match the given address in input text to addre...   \n",
       "2        Match the given address in input text to addre...   \n",
       "3        Match the given address in input text to addre...   \n",
       "4        Match the given address in input text to addre...   \n",
       "...                                                    ...   \n",
       "4542631  Find street in given address: 242 Z ST SE ,  ,...   \n",
       "4542632  Find street in given address: 201 Z ST SE ,  ,...   \n",
       "4542633  Find street in given address: 230 Z ST SE ,  ,...   \n",
       "4542634  Find street in given address: 709 Z ST SE ,  ,...   \n",
       "4542635  Find street in given address: 207 Z ST SE ,  ,...   \n",
       "\n",
       "                                               target_text  \n",
       "0        OUTPUT :  O'BRIEN & COMPANY , 811 1ST AVE STE ...  \n",
       "1        OUTPUT : O'BRIEN & COMPANY , 811 FIRST AVE SUI...  \n",
       "2        OUTPUT : O'BRIEN & COMPANY , 811 FIRST AVE SUI...  \n",
       "3        OUTPUT :  C/O  MUD BAY , 711 78TH AVE SW ,  , ...  \n",
       "4        OUTPUT : 711 78TH AVE SW ,  ,  , US , WA , TUM...  \n",
       "...                                                    ...  \n",
       "4542631                                   OUTPUT : Z_ST_SE  \n",
       "4542632                                   OUTPUT : Z_ST_SE  \n",
       "4542633                                   OUTPUT : Z_ST_SE  \n",
       "4542634                                   OUTPUT : Z_ST_SE  \n",
       "4542635                                   OUTPUT : Z_ST_SE  \n",
       "\n",
       "[5590211 rows x 2 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ans.to_csv(\"Final_Oct.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Development Environment\n",
    "\n",
    "Our first step is to install the Hugging Face Libraries, including transformers and datasets. Running the following cell will install all the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/1a/d1/3bba59606141ae808017f6fde91453882f931957f125009417b87a281067/transformers-4.34.0-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting rouge-score\n",
      "  Using cached rouge_score-0.1.2-py3-none-any.whl\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting tensorboard\n",
      "  Obtaining dependency information for tensorboard from https://files.pythonhosted.org/packages/73/a2/66ed644f6ed1562e0285fcd959af17670ea313c8f331c46f79ee77187eb9/tensorboard-2.14.1-py3-none-any.whl.metadata\n",
      "  Using cached tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting py7zr\n",
      "  Obtaining dependency information for py7zr from https://files.pythonhosted.org/packages/2c/da/155bb1f692c067b9213c9c7b8c19a012a65027399606d623a25dfb1d3af1/py7zr-0.20.6-py3-none-any.whl.metadata\n",
      "  Using cached py7zr-0.20.6-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pytesseract) (21.3)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pytesseract) (10.0.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/8f/3e/4b8b40eb3c80aeaf360f0361d956d129bb3d23b2a3ecbe3a04a8f3bdd6d3/regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/a7/7b/c1f643eb086b6c5c33eef0c3752e37624bd23e4cbc9f1332748f1c6252d1/tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/20/4e/878b080dbda92666233ec6f316a53969edcb58eab1aa399a64d0521cf953/safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/80/8a/1dd41557883b6196f8f092011a5c1f72d4d44cf36d7b67d4a5efe3127949/xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (2023.6.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/41/8e/4c48881316bbced3d13089c4d0df4be321ce79a0c695d82dee9996aaf56b/aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Obtaining dependency information for absl-py from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\n",
      "  Using cached absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk) (1.3.1)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Obtaining dependency information for grpcio>=1.48.2 from https://files.pythonhosted.org/packages/20/7f/e76618521aa9d33c6c1c9c3473f866da521678aa6ea2f4df3a896757748c/grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/39/7c/2e4fa55a99f83ef9ef229ac5d59c44ceb90e2d0145711590c0fa39669f32/google_auth-2.23.3-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_auth-2.23.3-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/bb/c1/50caaec6cadc1c6adc8fe351e03bd646d6e4dd17f55fca0f4c8d7ea8d3e9/Markdown-3.5-py3-none-any.whl.metadata\n",
      "  Using cached Markdown-3.5-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (4.23.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (68.0.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/02/52/fb9e51fba47951aabd7a6b25e41d73eae94208ccf62d886168096941a781/tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (2.3.6)\n",
      "Collecting texttable (from py7zr)\n",
      "  Obtaining dependency information for texttable from https://files.pythonhosted.org/packages/24/99/4772b8e00a136f3e01236de33b0efda31ee7077203ba5967fcc76da94d65/texttable-1.7.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting pycryptodomex>=3.6.6 (from py7zr)\n",
      "  Obtaining dependency information for pycryptodomex>=3.6.6 from https://files.pythonhosted.org/packages/23/23/3f3d042c96ff7bece5b126365593b1f9c8e3ae62ce80d44e9da39c5e8a73/pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting pyzstd>=0.14.4 (from py7zr)\n",
      "  Obtaining dependency information for pyzstd>=0.14.4 from https://files.pythonhosted.org/packages/02/97/57de14ccb0d033464aabc5110eb2f9dc717c97e17f2ffc1235680a27e5de/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting pyppmd<1.1.0,>=0.18.1 (from py7zr)\n",
      "  Using cached pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "Collecting pybcj>=0.6.0 (from py7zr)\n",
      "  Using cached pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
      "Collecting multivolumefile>=0.2.3 (from py7zr)\n",
      "  Using cached multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Collecting brotli>=1.0.9 (from py7zr)\n",
      "  Obtaining dependency information for brotli>=1.0.9 from https://files.pythonhosted.org/packages/d5/00/40f760cc27007912b327fe15bf6bfd8eaecbe451687f72a8abc587d503b3/Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata\n",
      "  Using cached Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting inflate64>=0.3.1 (from py7zr)\n",
      "  Using cached inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from py7zr) (5.9.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.7.2)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=21.3->pytesseract) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
      "Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
      "Using cached py7zr-0.20.6-py3-none-any.whl (66 kB)\n",
      "Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "Using cached Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
      "Using cached aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Using cached google_auth-2.23.3-py2.py3-none-any.whl (182 kB)\n",
      "Using cached grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "Using cached Markdown-3.5-py3-none-any.whl (101 kB)\n",
      "Using cached pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
      "Using cached regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "Using cached safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "Using cached tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "Using cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "Using cached texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "Installing collected packages: texttable, brotli, xxhash, tensorboard-data-server, safetensors, regex, pyzstd, pyppmd, pycryptodomex, pybcj, pyasn1-modules, oauthlib, multivolumefile, multidict, markdown, inflate64, grpcio, frozenlist, cachetools, async-timeout, absl-py, yarl, requests-oauthlib, pytesseract, py7zr, nltk, huggingface-hub, google-auth, aiosignal, tokenizers, rouge-score, google-auth-oauthlib, aiohttp, transformers, tensorboard, datasets\n",
      "Successfully installed absl-py-2.0.0 aiohttp-3.8.6 aiosignal-1.3.1 async-timeout-4.0.3 brotli-1.1.0 cachetools-5.3.1 datasets-2.14.5 frozenlist-1.4.0 google-auth-2.23.3 google-auth-oauthlib-1.0.0 grpcio-1.59.0 huggingface-hub-0.17.3 inflate64-0.3.1 markdown-3.5 multidict-6.0.4 multivolumefile-0.2.3 nltk-3.8.1 oauthlib-3.2.2 py7zr-0.20.6 pyasn1-modules-0.3.0 pybcj-1.0.1 pycryptodomex-3.19.0 pyppmd-1.0.0 pytesseract-0.3.10 pyzstd-0.15.9 regex-2023.10.3 requests-oauthlib-1.3.1 rouge-score-0.1.2 safetensors-0.4.0 tensorboard-2.14.1 tensorboard-data-server-0.7.1 texttable-1.7.0 tokenizers-0.14.1 transformers-4.34.0 xxhash-3.4.1 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "# python\n",
    "!pip install pytesseract transformers datasets rouge-score nltk tensorboard py7zr --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sudo: apt-get: command not found\r\n"
     ]
    }
   ],
   "source": [
    "# install git-fls for pushing model and logs to the hugging face hub\n",
    "!sudo apt-get install git-lfs --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will use the [Hugging Face Hub](https://huggingface.co/models) as a remote model versioning service. To be able to push our model to the Hub, you need to register on the [Hugging Face](https://huggingface.co/join). \n",
    "If you already have an account, you can skip this step. \n",
    "After you have an account, we will use the `notebook_login` util from the `huggingface_hub` package to log into our account and store our token (access key) on the disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv(\"FINAL_v2.csv\",on_bad_lines = 'skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e45e229f99744abbb93ff84e8ea7e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef387e271ce4839b837223d6ae25add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e556fc2c3aa411dabfe417cfa19940f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset, load_metric\n",
    "medium_datasets = load_dataset(\"csv\", data_files=\"FINAL_v2.csv\",cache_dir=\"~/SageMaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_train_test = medium_datasets[\"train\"].train_test_split(test_size=5000)\n",
    "datasets_train_validation = datasets_train_test[\"train\"].train_test_split(test_size=5000)\n",
    "medium_datasets[\"train\"] = datasets_train_validation[\"train\"]\n",
    "medium_datasets[\"validation\"] = datasets_train_validation[\"test\"]\n",
    "medium_datasets[\"test\"] = datasets_train_test[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training set: 99.82%\n",
      "- Validation set: 0.09%\n",
      "- Test set: 0.09%\n"
     ]
    }
   ],
   "source": [
    "n_samples_train = len(medium_datasets[\"train\"])\n",
    "n_samples_validation = len(medium_datasets[\"validation\"])\n",
    "n_samples_test = len(medium_datasets[\"test\"])\n",
    "n_samples_total = n_samples_train + n_samples_validation + n_samples_test\n",
    "\n",
    "print(f\"- Training set: {n_samples_train*100/n_samples_total:.2f}%\")\n",
    "print(f\"- Validation set: {n_samples_validation*100/n_samples_total:.2f}%\")\n",
    "print(f\"- Test set: {n_samples_test*100/n_samples_total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_text', 'target_text'],\n",
       "        num_rows: 5580211\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_text', 'target_text'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_text', 'target_text'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making shufffling of dataset\n",
    "medium_datasets[\"train\"] = medium_datasets[\"train\"].shuffle().select(range(n_samples_train))#131111260\n",
    "medium_datasets[\"validation\"] = medium_datasets[\"validation\"].shuffle().select(range(n_samples_validation))\n",
    "medium_datasets[\"test\"] = medium_datasets[\"test\"].shuffle().select(range(n_samples_test))\n",
    "medium_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = medium_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare samsum dataset\n",
    "\n",
    "we will use the [samsum](https://huggingface.co/datasets/samsum) dataset a collection of about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"13818513\",\n",
    "  \"summary\": \"Amanda baked cookies and will bring Jerry some tomorrow.\",\n",
    "  \"dialogue\": \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_id = \"samsum\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the `samsum` dataset, we use the `load_dataset()` method from the 🤗 Datasets library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# # Load dataset from the hub\n",
    "# dataset = load_dataset(dataset_id,cache_dir=\"SageMaker\")\n",
    "\n",
    "# print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "# print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "\n",
    "# # Train dataset size: 14732\n",
    "# # Test dataset size: 819"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets checkout an example of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_text', 'target_text'],\n",
       "        num_rows: 5580211\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_text', 'target_text'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_text', 'target_text'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_text', 'target_text'],\n",
       "    num_rows: 5580211\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue: \n",
      "correct address : address : JADELYN ALLCHIN , 4294 WHITMAN LN NE ,  , US , WA , SEATTLE , 98195-0047\n",
      "---------------\n",
      "summary: \n",
      "output : WHITMAN_LN_NE\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "from random import randrange        \n",
    "\n",
    "\n",
    "sample = dataset['train'][randrange(len(dataset[\"train\"]))]\n",
    "print(f\"dialogue: \\n{sample['input_text']}\\n---------------\")\n",
    "print(f\"summary: \\n{sample['target_text']}\\n---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model we need to convert our inputs (text) to token IDs. This is done by a 🤗 Transformers Tokenizer. If you are not sure what this means check out [chapter 6](https://huggingface.co/course/chapter6/1?fw=tf) of the Hugging Face Course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id=\"google/flan-t5-base\"\n",
    "\n",
    "# Load tokenizer of FLAN-t5-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,cache_dir=\"SageMaker\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before we can start training we need to preprocess our data. Abstractive Summarization is a text2text-generation task. This means our model will take a text as input and generate a summary as output. For this we want to understand how long our input and output will be to be able to efficiently batch our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb35466028cd450ba8bae546027e32b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5585211 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 254\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd872a29b854304b0e62740c473ec1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5585211 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 129\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# The maximum total input sequence length after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"input_text\"], truncation=True), batched=True, remove_columns=[\"input_text\", \"target_text\"])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"target_text\"], truncation=True), batched=True, remove_columns=[\"input_text\", \"target_text\"])\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68b4eb73f354900b5be61e5b6821b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5580211 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6ac925eb2c476480d4dd2e20152b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847879250f4d463f9b5b47d2c7ea361a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [item for item in sample[\"input_text\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"target_text\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"input_text\", \"target_text\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune and evaluate FLAN-T5\n",
    "\n",
    "After we have processed our dataset, we can start training our model. Therefore we first need to load our [FLAN-T5](https://huggingface.co/models?search=flan-t5) from the Hugging Face Hub. In the example we are using a instance with a NVIDIA V100 meaning that we will fine-tune the `base` version of the model. \n",
    "_I plan to do a follow-up post on how to fine-tune the `xxl` version of the model using Deepspeed._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# huggingface hub model id\n",
    "model_id=\"google/flan-t5-base\"\n",
    "\n",
    "# load model from the hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id,cache_dir=\"SageMaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to evaluate our model during training. The `Trainer` supports evaluation during training by providing a `compute_metrics`.  \n",
    "The most commonly used metrics to evaluate summarization task is [rogue_score](https://en.wikipedia.org/wiki/ROUGE_(metric)) short for Recall-Oriented Understudy for Gisting Evaluation). This metric does not behave like the standard accuracy: it will compare a generated summary against a set of reference summaries\n",
    "\n",
    "We are going to use `evaluate` library to evaluate the `rogue` score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start training is to create a `DataCollator` that will take care of padding our inputs and labels. We will use the `DataCollatorForSeq2Seq` from the 🤗 Transformers library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to define the hyperparameters (`TrainingArguments`) we want to use for our training. We are leveraging the [Hugging Face Hub](https://huggingface.co/models) integration of the `Trainer` to automatically push our checkpoints, logs and metrics during training into a repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Hugging Face repository id\n",
    "repository_id = f\"{model_id.split('/')[1]}-{dataset_id}\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # Overflows with fp16\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"overall_f1\",\n",
    "    # push to hub parameters\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=repository_id,\n",
    "    hub_token=HfFolder.get_token(),\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start our training by using the `train` method of the `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15238' max='87191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15238/87191 6:43:23 < 31:45:04, 0.63 it/s, Epoch 0.17/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/trainer.py:1870\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1867\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1870\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1871\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1872\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/accelerate/data_loader.py:394\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 394\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/datasets/arrow_dataset.py:2807\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m   2806\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2807\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2808\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[1;32m   2809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/datasets/arrow_dataset.py:2803\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2802\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/datasets/arrow_dataset.py:2788\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2786\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2787\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 2788\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2790\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2791\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/datasets/formatting/formatting.py:629\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    627\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/datasets/formatting/formatting.py:400\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/datasets/formatting/formatting.py:448\u001b[0m, in \u001b[0;36mPythonFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyBatch(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 448\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_batch(batch)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/datasets/formatting/formatting.py:150\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![flan-t5-tensorboard](../assets/flan-t5-tensorboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we have trained our model. 🎉 Lets run evaluate the best model again on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score we achieved is an `rouge1` score of `47.23`. \n",
    "\n",
    "Lets save our results and tokenizer to the Hugging Face Hub and create a model card. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our tokenizer and create model card\n",
    "tokenizer.save_pretrained(repository_id)\n",
    "#trainer.create_model_card()\n",
    "# Push the results to the hub\n",
    "#trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Inference\n",
    "\n",
    "Now we have a trained model, we can use it to run inference. We will use the `pipeline` API from transformers and a `test` example from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087baf2faf174209b6e917fcb4140311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1986b6c16ef41028848c3c3a97e0a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9bd85f75fa42a3a2641225308f8a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbdb03b0edf402b891d73eb3e5df04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8088fc844c1c447f838628b54e6b3d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66f9f679c414ec79c207a6ba92e6c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 127. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue: \n",
      "Richie: Pogba\n",
      "Clay: Pogboom\n",
      "Richie: what a s strike yoh!\n",
      "Clay: was off the seat the moment he chopped the ball back to his right foot\n",
      "Richie: me too dude\n",
      "Clay: hope his form lasts\n",
      "Richie: This season he's more mature\n",
      "Clay: Yeah, Jose has his trust in him\n",
      "Richie: everyone does\n",
      "Clay: yeah, he really deserved to score after his first 60 minutes\n",
      "Richie: reward\n",
      "Clay: yeah man\n",
      "Richie: cool then \n",
      "Clay: cool\n",
      "---------------\n",
      "flan-t5-base summary:\n",
      "Pogba scored a strike after his first 60 minutes. Richie and Clay hope his form lasts this season and he's more mature.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import pipeline\n",
    "# from random import randrange        \n",
    "\n",
    "# # load model and tokenizer from huggingface hub with pipeline\n",
    "# summarizer = pipeline(\"summarization\", model=\"philschmid/flan-t5-base-samsum\", device=0)\n",
    "\n",
    "# # select a random test sample\n",
    "# sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "# print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "\n",
    "# # summarize dialogue\n",
    "# res = summarizer(sample[\"dialogue\"])\n",
    "\n",
    "# print(f\"flan-t5-base summary:\\n{res[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
